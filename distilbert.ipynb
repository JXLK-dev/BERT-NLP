{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jerry\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import re\n",
    "import unicodedata\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense,Dropout, Input\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from sklearn.metrics import confusion_matrix,f1_score,classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "from sklearn.utils import shuffle\n",
    "from tensorflow.keras import regularizers\n",
    "from transformers import BertTokenizer, TFBertModel, BertConfig,TFDistilBertModel,DistilBertTokenizer,DistilBertConfig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jerry\\AppData\\Local\\Temp\\ipykernel_13184\\2875405556.py:3: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def read_data():\n",
      "C:\\Users\\Jerry\\AppData\\Local\\Temp\\ipykernel_13184\\2875405556.py:2: NumbaWarning: \u001b[1m\n",
      "Compilation is falling back to object mode WITH looplifting enabled because Function \"read_data\" failed type inference due to: \u001b[1m\u001b[1mUnknown attribute 'read_csv' of type Module(<module 'pandas' from 'c:\\\\Users\\\\Jerry\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\lib\\\\site-packages\\\\pandas\\\\__init__.py'>)\n",
      "\u001b[1m\n",
      "File \"..\\..\\..\\..\\..\\AppData\\Local\\Temp\\ipykernel_13184\\2875405556.py\", line 4:\u001b[0m\n",
      "\u001b[1m<source missing, REPL/exec in use?>\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[1mDuring: typing of get attribute at C:\\Users\\Jerry\\AppData\\Local\\Temp\\ipykernel_13184\\2875405556.py (4)\u001b[0m\n",
      "\u001b[1m\n",
      "File \"..\\..\\..\\..\\..\\AppData\\Local\\Temp\\ipykernel_13184\\2875405556.py\", line 4:\u001b[0m\n",
      "\u001b[1m<source missing, REPL/exec in use?>\u001b[0m\n",
      "\u001b[0m\n",
      "  @jit\n",
      "c:\\Users\\Jerry\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numba\\core\\object_mode_passes.py:151: NumbaWarning: \u001b[1mFunction \"read_data\" was compiled in object mode without forceobj=True.\n",
      "\u001b[1m\n",
      "File \"..\\..\\..\\..\\..\\AppData\\Local\\Temp\\ipykernel_13184\\2875405556.py\", line 2:\u001b[0m\n",
      "\u001b[1m<source missing, REPL/exec in use?>\u001b[0m\n",
      "\u001b[0m\n",
      "  warnings.warn(errors.NumbaWarning(warn_msg,\n",
      "c:\\Users\\Jerry\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numba\\core\\object_mode_passes.py:161: NumbaDeprecationWarning: \u001b[1m\n",
      "Fall-back from the nopython compilation path to the object mode compilation path has been detected. This is deprecated behaviour that will be removed in Numba 0.59.0.\n",
      "\n",
      "For more information visit https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit\n",
      "\u001b[1m\n",
      "File \"..\\..\\..\\..\\..\\AppData\\Local\\Temp\\ipykernel_13184\\2875405556.py\", line 2:\u001b[0m\n",
      "\u001b[1m<source missing, REPL/exec in use?>\u001b[0m\n",
      "\u001b[0m\n",
      "  warnings.warn(errors.NumbaDeprecationWarning(msg,\n"
     ]
    }
   ],
   "source": [
    "from numba import jit,cuda\n",
    "@jit\n",
    "def read_data():\n",
    "    test_csv = pd.read_csv('datasets/cleaned_test_data.csv')\n",
    "    train_csv = pd.read_csv('datasets/clean_train.csv')\n",
    "    test_csv = test_csv.drop(test_csv.index[0])\n",
    "    return train_csv,test_csv\n",
    "df_train,df_test = read_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_balanced = df_train[df_train['class']==1].sample(100)\n",
    "for index in range(2,11):\n",
    "    df_balanced = pd.concat([df_balanced,df_train[df_train['class']==index].sample(100)])\n",
    "x_train = df_balanced['text']\n",
    "labels = df_balanced['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertModel: ['vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing TFDistilBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFDistilBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "max_len=256\n",
    "dbert_tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "dbert_model = TFDistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "def create_model():\n",
    "    inps = Input(shape = (max_len,), dtype='int64')\n",
    "    masks= Input(shape = (max_len,), dtype='int64')\n",
    "    dbert_layer = dbert_model(inps, attention_mask=masks)[0][:,0,:]\n",
    "    dense = Dense(512,activation='relu',kernel_regularizer=regularizers.l2(0.01))(dbert_layer)\n",
    "    dropout= Dropout(0.5)(dense)\n",
    "    pred = Dense(11, activation='softmax',kernel_regularizer=regularizers.l2(0.01))(dropout)\n",
    "    model = tf.keras.Model(inputs=[inps,masks], outputs=pred)\n",
    "    print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the saved pickle files..\n",
      "Input shape (20000, 32) Attention mask shape (20000, 32) Input label shape (20000,)\n"
     ]
    }
   ],
   "source": [
    "print('Loading the saved pickle files..')\n",
    "pickle_inp_path='./data/dbert_inp.pkl'\n",
    "pickle_mask_path='./data/dbert_mask.pkl'\n",
    "pickle_label_path='./data/dbert_label.pkl'\n",
    "input_ids=pickle.load(open(pickle_inp_path, 'rb'))\n",
    "attention_masks=pickle.load(open(pickle_mask_path, 'rb'))\n",
    "labels=pickle.load(open(pickle_label_path, 'rb'))\n",
    "print('Input shape {} Attention mask shape {} Input label shape {}'.format(input_ids.shape,attention_masks.shape,labels.shape))\n",
    "train_inp,val_inp,train_label,val_label,train_mask,val_mask=train_test_split(input_ids,labels,attention_masks,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_6 (InputLayer)           [(None, 256)]        0           []                               \n",
      "                                                                                                  \n",
      " input_7 (InputLayer)           [(None, 256)]        0           []                               \n",
      "                                                                                                  \n",
      " tf_distil_bert_model_4 (TFDist  TFBaseModelOutput(l  66362880   ['input_6[0][0]',                \n",
      " ilBertModel)                   ast_hidden_state=(N               'input_7[0][0]']                \n",
      "                                one, 256, 768),                                                   \n",
      "                                 hidden_states=None                                               \n",
      "                                , attentions=None)                                                \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_1 (Sl  (None, 768)         0           ['tf_distil_bert_model_4[0][0]'] \n",
      " icingOpLambda)                                                                                   \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 512)          393728      ['tf.__operators__.getitem_1[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " dropout_96 (Dropout)           (None, 512)          0           ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 11)           5643        ['dropout_96[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 66,762,251\n",
      "Trainable params: 66,762,251\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model_save_path='./dbert_model.h5'\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
    "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "trained_model = create_model()\n",
    "trained_model.compile(loss=loss,optimizer=optimizer, metrics=[metric])\n",
    "trained_model.load_weights(model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 238ms/step\n",
      "input_id: [[ 101 2065 2396 2003 2000 2053 9496 4095 1996 6147 1997 2256 3226  102\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0]]\n",
      "attention_mask: [[1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0]]\n",
      "prediction: [[0.00439014 0.8174227  0.01700242 0.00617047 0.08479136 0.00183737\n",
      "  0.00870188 0.01489065 0.02180386 0.00627383 0.01671522]]\n",
      "Society & Culture\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "while(True):\n",
    "    clear_output(wait=True)\n",
    "    string = input()\n",
    "    if(string == 'exit'):break\n",
    "    input_list = []\n",
    "    mask_list=[]\n",
    "    dbert_inps=dbert_tokenizer.encode_plus(string,add_special_tokens = True,max_length =max_len,pad_to_max_length = True,return_attention_mask = True,truncation=True)\n",
    "    input_list.append(dbert_inps['input_ids'])\n",
    "    mask_list.append(dbert_inps['attention_mask'])\n",
    "    input_id = np.array(input_list)\n",
    "    mask = np.array(mask_list)\n",
    "    preds = trained_model.predict([input_id,mask],batch_size=16)\n",
    "    print(f\"input_id: {input_id}\")\n",
    "    print(f\"attention_mask: {mask}\")\n",
    "    print(f\"prediction: {preds}\")\n",
    "    pred_labels = preds.argmax(axis=1)\n",
    "    testing = {1: 'Society & Culture', 2: 'Science & Mathematics', 3: 'Health', 4: 'Education & Reference',\n",
    "                5: 'Computers & Internet', 6: 'Sports', 7: 'Business & Finance', 8: 'Entertainment & Music',\n",
    "                9: 'Family & Relationships', 10: 'Politics & Government'}\n",
    "    print(testing[pred_labels[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.models.distilbert.tokenization_distilbert.DistilBertTokenizer"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dbert_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 106s 419ms/step\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.84      0.83      0.83       408\n",
      "           2       0.95      0.87      0.91       416\n",
      "           3       0.94      0.92      0.93       409\n",
      "           4       0.83      0.85      0.84       385\n",
      "           5       0.94      0.96      0.95       394\n",
      "           6       0.97      0.95      0.96       403\n",
      "           7       0.85      0.71      0.78       397\n",
      "           8       0.82      0.90      0.86       382\n",
      "           9       0.82      0.94      0.88       399\n",
      "          10       0.91      0.95      0.93       407\n",
      "\n",
      "    accuracy                           0.89      4000\n",
      "   macro avg       0.89      0.89      0.89      4000\n",
      "weighted avg       0.89      0.89      0.89      4000\n",
      "\n",
      "Training and saving built model.....\n"
     ]
    }
   ],
   "source": [
    "preds = trained_model.predict([val_inp,val_mask],batch_size=16)\n",
    "pred_labels = preds.argmax(axis=1)\n",
    "print('Classification Report')\n",
    "print(classification_report(val_label,pred_labels))\n",
    "print('Training and saving built model.....')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.regularization.dropout.Dropout object at 0x000001C18DF38130>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.regularization.dropout.Dropout object at 0x000001C184CCC040>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.regularization.dropout.Dropout object at 0x000001C18DFE1720>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.regularization.dropout.Dropout object at 0x000001C18BB9E500>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.regularization.dropout.Dropout object at 0x000001C18BB40D00>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.regularization.dropout.Dropout object at 0x000001C1F5BEC9D0>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as embeddings_layer_call_fn, embeddings_layer_call_and_return_conditional_losses, transformer_layer_call_fn, transformer_layer_call_and_return_conditional_losses, LayerNorm_layer_call_fn while saving (showing 5 of 164). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\Jerry\\AppData\\Local\\Temp\\tmpoeskzeh2\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\Jerry\\AppData\\Local\\Temp\\tmpoeskzeh2\\assets\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "266415768"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(trained_model)\n",
    "tflite_model = converter.convert()\n",
    "open(\"distilbert_slim_model.tflite\",\"wb\").write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  101,  2175,  6998,  3789,  3924,  6830,  2156,  6433,  3134,\n",
       "        2051,  1029,  3437, 14477,  3619, 13777,  2094,  3980,  3789,\n",
       "        3437,  2190,  2156,  4148,  3134,  2051,  1012,  1012,  1032,\n",
       "        1050,  1032,  1050,  2903,   102])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_inp[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nt optical mouse work glass table  even surfac...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>best offroad motorcycle trail  longdistance tr...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>trans fat  reduce  heard tras fat bad body   f...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>many planes fede  heard largest airline world ...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>san francisco bay area   make sense rent buy  ...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>768320</th>\n",
       "      <td>believe hopelessness  believe religion   lack ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>768321</th>\n",
       "      <td>get horse s skeletal muscular systems web prin...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>768322</th>\n",
       "      <td>quest promote racial equality government media...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>768323</th>\n",
       "      <td>ways sell video games  like want sell video ga...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>768324</th>\n",
       "      <td>speak hindi   write   main hindi bol sakti hoo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>768325 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  class\n",
       "0       nt optical mouse work glass table  even surfac...      5\n",
       "1       best offroad motorcycle trail  longdistance tr...      6\n",
       "2       trans fat  reduce  heard tras fat bad body   f...      3\n",
       "3       many planes fede  heard largest airline world ...      7\n",
       "4       san francisco bay area   make sense rent buy  ...      7\n",
       "...                                                   ...    ...\n",
       "768320  believe hopelessness  believe religion   lack ...      1\n",
       "768321  get horse s skeletal muscular systems web prin...      4\n",
       "768322  quest promote racial equality government media...     10\n",
       "768323  ways sell video games  like want sell video ga...      7\n",
       "768324  speak hindi   write   main hindi bol sakti hoo...      1\n",
       "\n",
       "[768325 rows x 2 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# categories = {'Society & Culture' :1,'Science & Mathematics':2,'Health':3, 'Education & Reference':4,\n",
    "#             'Computers & Internet' :5,'Sports' :6,'Business & Finance' :7,'Entertainment & Music' : 8,\n",
    "#             'Family & Relationships':9, 'Politics & Government':10}\n",
    "# df_train['class']= df_train['class'].map(categories)\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertModel: ['vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing TFDistilBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFDistilBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "dbert_tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "dbert_model = TFDistilBertModel.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = df_balanced['text']\n",
    "y_train = df_balanced['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len=256\n",
    "def create_model():\n",
    "    inps = Input(shape = (max_len,), dtype='int64')\n",
    "    masks= Input(shape = (max_len,), dtype='int64')\n",
    "    dbert_layer = dbert_model(inps, attention_mask=masks)[0][:,0,:]\n",
    "    dense = Dense(512,activation='relu',kernel_regularizer=regularizers.l2(0.01))(dbert_layer)\n",
    "    dropout= Dropout(0.5)(dense)\n",
    "    pred = Dense(11, activation='softmax',kernel_regularizer=regularizers.l2(0.01))(dropout)\n",
    "    model = tf.keras.Model(inputs=[inps,masks], outputs=pred)\n",
    "    print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 256)]        0           []                               \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 256)]        0           []                               \n",
      "                                                                                                  \n",
      " tf_distil_bert_model (TFDistil  TFBaseModelOutput(l  66362880   ['input_1[0][0]',                \n",
      " BertModel)                     ast_hidden_state=(N               'input_2[0][0]']                \n",
      "                                one, 256, 768),                                                   \n",
      "                                 hidden_states=None                                               \n",
      "                                , attentions=None)                                                \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem (Slic  (None, 768)         0           ['tf_distil_bert_model[0][0]']   \n",
      " ingOpLambda)                                                                                     \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 512)          393728      ['tf.__operators__.getitem[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " dropout_19 (Dropout)           (None, 512)          0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 11)           5643        ['dropout_19[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 66,762,251\n",
      "Trainable params: 66,762,251\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jerry\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "input_ids=[]\n",
    "attention_masks=[]\n",
    "\n",
    "for sent in x_train:\n",
    "    dbert_inps=dbert_tokenizer.encode_plus(sent,add_special_tokens = True,max_length =max_len,pad_to_max_length = True,return_attention_mask = True,truncation=True)\n",
    "    input_ids.append(dbert_inps['input_ids'])\n",
    "    attention_masks.append(dbert_inps['attention_mask'])\n",
    "\n",
    "input_ids=np.asarray(input_ids)\n",
    "attention_masks=np.array(attention_masks)\n",
    "labels=np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing the pickle file.....\n",
      "Pickle files saved as  ./data/dbert_inp.pkl ./data/dbert_mask.pkl ./data/dbert_label.pkl\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('Preparing the pickle file.....')\n",
    "\n",
    "pickle_inp_path='./data/dbert_inp.pkl'\n",
    "pickle_mask_path='./data/dbert_mask.pkl'\n",
    "pickle_label_path='./data/dbert_label.pkl'\n",
    "\n",
    "pickle.dump((input_ids),open(pickle_inp_path,'wb'))\n",
    "pickle.dump((attention_masks),open(pickle_mask_path,'wb'))\n",
    "pickle.dump((labels),open(pickle_label_path,'wb'))\n",
    "\n",
    "\n",
    "print('Pickle files saved as ',pickle_inp_path,pickle_mask_path,pickle_label_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_inp_path='./data/dbert_inp.pkl'\n",
    "pickle_mask_path='./data/dbert_mask.pkl'\n",
    "pickle_label_path='./data/dbert_label.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the saved pickle files..\n",
      "Input shape (20000, 32) Attention mask shape (20000, 32) Input label shape (20000,)\n"
     ]
    }
   ],
   "source": [
    "print('Loading the saved pickle files..')\n",
    "pickle_inp_path='./data/dbert_inp.pkl'\n",
    "pickle_mask_path='./data/dbert_mask.pkl'\n",
    "pickle_label_path='./data/dbert_label.pkl'\n",
    "input_ids=pickle.load(open(pickle_inp_path, 'rb'))\n",
    "attention_masks=pickle.load(open(pickle_mask_path, 'rb'))\n",
    "labels=pickle.load(open(pickle_label_path, 'rb'))\n",
    "\n",
    "print('Input shape {} Attention mask shape {} Input label shape {}'.format(input_ids.shape,attention_masks.shape,labels.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir='dbert_model_new'\n",
    "model_save_path='./dbert_model.h5'\n",
    "\n",
    "callbacks = [tf.keras.callbacks.ModelCheckpoint(filepath=model_save_path,save_weights_only=True,monitor='val_loss',mode='min',save_best_only=True),keras.callbacks.TensorBoard(log_dir=log_dir)]\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
    "callbacks= [tf.keras.callbacks.ModelCheckpoint(filepath=model_save_path,save_weights_only=True,monitor='val_loss',mode='min',save_best_only=True),keras.callbacks.TensorBoard(log_dir=log_dir)]\n",
    "model.compile(loss=loss,optimizer=optimizer, metrics=[metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inp,val_inp,train_label,val_label,train_mask,val_mask=train_test_split(input_ids,labels,attention_masks,test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jerry\\AppData\\Local\\Temp\\ipykernel_13184\\1279885436.py:3: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def modelFitting():\n",
      "C:\\Users\\Jerry\\AppData\\Local\\Temp\\ipykernel_13184\\1279885436.py:2: NumbaWarning: \u001b[1m\n",
      "Compilation is falling back to object mode WITH looplifting enabled because Function \"modelFitting\" failed type inference due to: \u001b[1mUntyped global name 'history':\u001b[0m \u001b[1m\u001b[1mCannot determine Numba type of <class 'keras.engine.functional.Functional'>\u001b[0m\n",
      "\u001b[1m\n",
      "File \"..\\..\\..\\..\\..\\AppData\\Local\\Temp\\ipykernel_13184\\1279885436.py\", line 4:\u001b[0m\n",
      "\u001b[1m<source missing, REPL/exec in use?>\u001b[0m\n",
      "\u001b[0m\u001b[0m\n",
      "  @jit\n",
      "c:\\Users\\Jerry\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numba\\core\\object_mode_passes.py:151: NumbaWarning: \u001b[1mFunction \"modelFitting\" was compiled in object mode without forceobj=True.\n",
      "\u001b[1m\n",
      "File \"..\\..\\..\\..\\..\\AppData\\Local\\Temp\\ipykernel_13184\\1279885436.py\", line 2:\u001b[0m\n",
      "\u001b[1m<source missing, REPL/exec in use?>\u001b[0m\n",
      "\u001b[0m\n",
      "  warnings.warn(errors.NumbaWarning(warn_msg,\n",
      "c:\\Users\\Jerry\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numba\\core\\object_mode_passes.py:161: NumbaDeprecationWarning: \u001b[1m\n",
      "Fall-back from the nopython compilation path to the object mode compilation path has been detected. This is deprecated behaviour that will be removed in Numba 0.59.0.\n",
      "\n",
      "For more information visit https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit\n",
      "\u001b[1m\n",
      "File \"..\\..\\..\\..\\..\\AppData\\Local\\Temp\\ipykernel_13184\\1279885436.py\", line 2:\u001b[0m\n",
      "\u001b[1m<source missing, REPL/exec in use?>\u001b[0m\n",
      "\u001b[0m\n",
      "  warnings.warn(errors.NumbaDeprecationWarning(msg,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jerry\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\backend.py:5582: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n",
      "  output, from_logits = _get_logits(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/50 [==============================] - 444s 9s/step - loss: 8.5395 - accuracy: 0.1775 - val_loss: 7.9912 - val_accuracy: 0.4200\n",
      "Epoch 2/5\n",
      "50/50 [==============================] - 462s 9s/step - loss: 7.4774 - accuracy: 0.5813 - val_loss: 7.3016 - val_accuracy: 0.6500\n",
      "Epoch 3/5\n",
      "50/50 [==============================] - 488s 10s/step - loss: 6.8071 - accuracy: 0.7650 - val_loss: 7.0464 - val_accuracy: 0.6650\n",
      "Epoch 4/5\n",
      "50/50 [==============================] - 474s 9s/step - loss: 6.4278 - accuracy: 0.8575 - val_loss: 7.1091 - val_accuracy: 0.6750\n",
      "Epoch 5/5\n",
      "50/50 [==============================] - 482s 10s/step - loss: 6.1467 - accuracy: 0.9038 - val_loss: 7.0391 - val_accuracy: 0.6550\n"
     ]
    }
   ],
   "source": [
    "history=model\n",
    "@jit\n",
    "def modelFitting():\n",
    "    history.fit([train_inp,train_mask],train_label,batch_size=16,epochs=5,validation_data=([val_inp,val_mask],val_label),callbacks=callbacks)\n",
    "modelFitting()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = df_test['text']\n",
    "y_test = df_test['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jerry\\AppData\\Local\\Temp\\ipykernel_24472\\2584200295.py:4: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def clean_df():\n",
      "C:\\Users\\Jerry\\AppData\\Local\\Temp\\ipykernel_24472\\2584200295.py:3: NumbaWarning: \u001b[1m\n",
      "Compilation is falling back to object mode WITH looplifting enabled because Function \"clean_df\" failed type inference due to: \u001b[1mUntyped global name 'df_clean':\u001b[0m \u001b[1m\u001b[1mCannot determine Numba type of <class 'pandas.core.frame.DataFrame'>\u001b[0m\n",
      "\u001b[1m\n",
      "File \"..\\..\\..\\..\\..\\AppData\\Local\\Temp\\ipykernel_24472\\2584200295.py\", line 5:\u001b[0m\n",
      "\u001b[1m<source missing, REPL/exec in use?>\u001b[0m\n",
      "\u001b[0m\u001b[0m\n",
      "  @jit\n",
      "c:\\Users\\Jerry\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numba\\core\\object_mode_passes.py:151: NumbaWarning: \u001b[1mFunction \"clean_df\" was compiled in object mode without forceobj=True.\n",
      "\u001b[1m\n",
      "File \"..\\..\\..\\..\\..\\AppData\\Local\\Temp\\ipykernel_24472\\2584200295.py\", line 3:\u001b[0m\n",
      "\u001b[1m<source missing, REPL/exec in use?>\u001b[0m\n",
      "\u001b[0m\n",
      "  warnings.warn(errors.NumbaWarning(warn_msg,\n",
      "c:\\Users\\Jerry\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numba\\core\\object_mode_passes.py:161: NumbaDeprecationWarning: \u001b[1m\n",
      "Fall-back from the nopython compilation path to the object mode compilation path has been detected. This is deprecated behaviour that will be removed in Numba 0.59.0.\n",
      "\n",
      "For more information visit https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit\n",
      "\u001b[1m\n",
      "File \"..\\..\\..\\..\\..\\AppData\\Local\\Temp\\ipykernel_24472\\2584200295.py\", line 3:\u001b[0m\n",
      "\u001b[1m<source missing, REPL/exec in use?>\u001b[0m\n",
      "\u001b[0m\n",
      "  warnings.warn(errors.NumbaDeprecationWarning(msg,\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "df_clean=df_train\n",
    "@jit\n",
    "def clean_df():\n",
    "    df_clean['text'] = df_clean['text'].map(clean_text)\n",
    "clean_df()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.to_csv('datasets/clean_train.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "        text: a string\n",
    "        \n",
    "        return: modified initial string\n",
    "    \"\"\"\n",
    "    text = text.lower() # lowercase text\n",
    "    text = REPLACE_BY_SPACE_RE.sub(' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text. substitute the matched string in REPLACE_BY_SPACE_RE with space.\n",
    "    text = BAD_SYMBOLS_RE.sub('', text) # remove symbols which are in BAD_SYMBOLS_RE from text. substitute the matched string in BAD_SYMBOLS_RE with nothing. \n",
    "    text = text.replace('x', '')\n",
    "#    text = re.sub(r'\\W+', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['people still offended michael richards remarks  ok   maybe s racist definetely nt said things said damn   get  even racist   hypothetical     going impact people offended  yeah',\n",
       "       'question christians  jesus reworked pagan myth early 1st century jews   secular evidence lived crucified ',\n",
       "       'ever sing loud  ever dance street  ndo ever say cheery  hello  stranger  nwhat result  think best thing know try  s nothing best  first hand eperience  know    smile  ',\n",
       "       ...,\n",
       "       'question people believe last question please answer  read  tell feel  honest opinions helpthis still happening believe notnsoldiers stories n   2 700 could fulfill full 36month tour duty   ve got pay back advance nt finish   wwwhendersoncountyncdemocratsorg soldiers_storieshtml  53k  cached  site simple accounting screwup  nnif gone military first instead news media would fied right away ',\n",
       "       'prop 83 mess california  voters epect rehabilatated se offenders kicked immediately voting   apply future se offenders released prison  nnfor detailnhttp    wwwkfmbcom printable   id69544n federal judge wednesday blocked enforcement key provisions proposition 83   ballot measure passed overwhelmingly voters s meant crack se offenders   including limiting may livennus  district judge susan illston   ruling lawsuit filed day election   said measure  punitive design effect  likely unconstitutionalnnthe socalled jessica s law prohibits registered se offenders living within 2 000 feet school park  effectively prohibiting parolees living many california s cities  s challenged housing restrictions  bill length sentences child molesters already signed law  authors already backtracked admitted made mistake nt write initiative clearly enough apply retroactively  may fly states like new york   new jersey   iowa georgia   california    may fully blue state re far red state  take constitutional rights seriously   many pissed people fighting initiative court  nnthe california department corrections also real happy parolees able live within 2000 feet schools parks  saying might build housing public epense house  something like  132 million start  state already multi billion budget   governor nt solve budget problem  borrowed funds floating bonds    nnalthough 70 oercent voters cast yes votes   courts frown initiatives popular unconstitutional  another eample initiative banned public services illegal aliens 1994  public voted   courts threw  nni agree 2nd poster  time legislators stop trying legislate kinds laws  look like idiots   especially ca nt even write initiatives rightnnalso link los angeles times editorial proposition 83  nnhttp    wwwlatimescom news opinion laedjessica12nov12 0 4249584story  colllaopinionleftrail',\n",
       "       'evict without taking court  read website tenant rights illegal lease say anything evicting immediatly   landlord bring court order evict   looked lease nt say anything court   says tell leave without prior notice   anyone know s legal  website go would give accurate information tenant laws  need something back  landlord   tenant laws vary greatly state state   even city city  places deal landlord ca nt engage  selfhelp      locking residence throwing things street   without court order  crazy states   like teas   allow landlord take  landlord s lien    personal property   may allow take possession stuff security failure pay rentnnhere s link article freezeouts findlawcom   reputable source s used attorneys average citizens alike  nhttp    realestatefindlawcom landlord landlordeviction tenantevictionillegalselfhelphtmlnnhere s another website based ohio law  sounds similar processes many states   may may state  nhttp    wwwohiolandlordtenantcom faq2html'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import pad_sequences\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\n",
    "tokenizer.fit_on_texts(x_train.values)\n",
    "X = tokenizer.texts_to_sequences(x_train.values)\n",
    "X = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "Y = pd.get_dummies(y_train).values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.10, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(180000, 10)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Embedding,SpatialDropout1D, LSTM\n",
    "MAX_NB_WORDS = 50000\n",
    "MAX_SEQUENCE_LENGTH = 250\n",
    "EMBEDDING_DIM = 100\n",
    "model = Sequential()\n",
    "model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X.shape[1]))\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jerry\\AppData\\Local\\Temp\\ipykernel_24472\\2224690945.py:6: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def modelFitting():\n",
      "C:\\Users\\Jerry\\AppData\\Local\\Temp\\ipykernel_24472\\2224690945.py:5: NumbaWarning: \u001b[1m\n",
      "Compilation is falling back to object mode WITH looplifting enabled because Function \"modelFitting\" failed type inference due to: \u001b[1mUntyped global name 'history':\u001b[0m \u001b[1m\u001b[1mCannot determine Numba type of <class 'keras.engine.sequential.Sequential'>\u001b[0m\n",
      "\u001b[1m\n",
      "File \"..\\..\\..\\..\\..\\AppData\\Local\\Temp\\ipykernel_24472\\2224690945.py\", line 7:\u001b[0m\n",
      "\u001b[1m<source missing, REPL/exec in use?>\u001b[0m\n",
      "\u001b[0m\u001b[0m\n",
      "  @jit\n",
      "c:\\Users\\Jerry\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numba\\core\\object_mode_passes.py:151: NumbaWarning: \u001b[1mFunction \"modelFitting\" was compiled in object mode without forceobj=True.\n",
      "\u001b[1m\n",
      "File \"..\\..\\..\\..\\..\\AppData\\Local\\Temp\\ipykernel_24472\\2224690945.py\", line 5:\u001b[0m\n",
      "\u001b[1m<source missing, REPL/exec in use?>\u001b[0m\n",
      "\u001b[0m\n",
      "  warnings.warn(errors.NumbaWarning(warn_msg,\n",
      "c:\\Users\\Jerry\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numba\\core\\object_mode_passes.py:161: NumbaDeprecationWarning: \u001b[1m\n",
      "Fall-back from the nopython compilation path to the object mode compilation path has been detected. This is deprecated behaviour that will be removed in Numba 0.59.0.\n",
      "\n",
      "For more information visit https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit\n",
      "\u001b[1m\n",
      "File \"..\\..\\..\\..\\..\\AppData\\Local\\Temp\\ipykernel_24472\\2224690945.py\", line 5:\u001b[0m\n",
      "\u001b[1m<source missing, REPL/exec in use?>\u001b[0m\n",
      "\u001b[0m\n",
      "  warnings.warn(errors.NumbaDeprecationWarning(msg,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "254/254 [==============================] - 293s 1s/step - loss: 1.9050 - accuracy: 0.3202 - val_loss: 1.5366 - val_accuracy: 0.4800\n",
      "Epoch 2/5\n",
      "254/254 [==============================] - 306s 1s/step - loss: 1.1295 - accuracy: 0.6278 - val_loss: 1.3392 - val_accuracy: 0.5683\n",
      "Epoch 3/5\n",
      "254/254 [==============================] - 301s 1s/step - loss: 0.7602 - accuracy: 0.7665 - val_loss: 1.2250 - val_accuracy: 0.6378\n",
      "Epoch 4/5\n",
      "254/254 [==============================] - 310s 1s/step - loss: 0.4725 - accuracy: 0.8618 - val_loss: 1.3652 - val_accuracy: 0.6183\n",
      "Epoch 5/5\n",
      "254/254 [==============================] - 315s 1s/step - loss: 0.3210 - accuracy: 0.9101 - val_loss: 1.3908 - val_accuracy: 0.6272\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "epochs = 5\n",
    "batch_size = 64\n",
    "history = model\n",
    "@jit\n",
    "def modelFitting():\n",
    "    history.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size,validation_split=0.1,callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)])\n",
    "modelFitting()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 5s 73ms/step - loss: 1.4505 - accuracy: 0.6135\n",
      "Test set\n",
      "  Loss: 1.451\n",
      "  Accuracy: 0.613\n"
     ]
    }
   ],
   "source": [
    "accr = model.evaluate(X_test,Y_test)\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  101,  9530, 21163, ...,  7419,  5350,   102],\n",
       "       [  101,  5788, 29500, ...,  3132, 27668,   102],\n",
       "       [  101,  6898,  2183, ..., 17612,  2015,   102],\n",
       "       ...,\n",
       "       [  101,  1029,  2003, ...,  5604,  4255,   102],\n",
       "       [  101,  6912,  3160, ...,  9157,  6187,   102],\n",
       "       [  101,  3728,  2318, ...,  2204, 18012,   102]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import SimpleRNN\n",
    "model = Sequential()\n",
    "model.add(Embedding(MAX_NB_WORDS, 24, input_length=X.shape[1]))\n",
    "model.add(SimpleRNN(24, return_sequences=False))\n",
    "model.add(Dense(10, activation='sigmoid'))\n",
    "\n",
    "# compile the model\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jerry\\AppData\\Local\\Temp\\ipykernel_24472\\4164863293.py:6: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def modelRNN():\n",
      "C:\\Users\\Jerry\\AppData\\Local\\Temp\\ipykernel_24472\\4164863293.py:5: NumbaWarning: \u001b[1m\n",
      "Compilation is falling back to object mode WITH looplifting enabled because Function \"modelRNN\" failed type inference due to: \u001b[1mUntyped global name 'history':\u001b[0m \u001b[1m\u001b[1mCannot determine Numba type of <class 'keras.engine.sequential.Sequential'>\u001b[0m\n",
      "\u001b[1m\n",
      "File \"..\\..\\..\\..\\..\\AppData\\Local\\Temp\\ipykernel_24472\\4164863293.py\", line 7:\u001b[0m\n",
      "\u001b[1m<source missing, REPL/exec in use?>\u001b[0m\n",
      "\u001b[0m\u001b[0m\n",
      "  @jit\n",
      "c:\\Users\\Jerry\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numba\\core\\object_mode_passes.py:151: NumbaWarning: \u001b[1mFunction \"modelRNN\" was compiled in object mode without forceobj=True.\n",
      "\u001b[1m\n",
      "File \"..\\..\\..\\..\\..\\AppData\\Local\\Temp\\ipykernel_24472\\4164863293.py\", line 5:\u001b[0m\n",
      "\u001b[1m<source missing, REPL/exec in use?>\u001b[0m\n",
      "\u001b[0m\n",
      "  warnings.warn(errors.NumbaWarning(warn_msg,\n",
      "c:\\Users\\Jerry\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numba\\core\\object_mode_passes.py:161: NumbaDeprecationWarning: \u001b[1m\n",
      "Fall-back from the nopython compilation path to the object mode compilation path has been detected. This is deprecated behaviour that will be removed in Numba 0.59.0.\n",
      "\n",
      "For more information visit https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit\n",
      "\u001b[1m\n",
      "File \"..\\..\\..\\..\\..\\AppData\\Local\\Temp\\ipykernel_24472\\4164863293.py\", line 5:\u001b[0m\n",
      "\u001b[1m<source missing, REPL/exec in use?>\u001b[0m\n",
      "\u001b[0m\n",
      "  warnings.warn(errors.NumbaDeprecationWarning(msg,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5625/5625 [==============================] - 229s 41ms/step - loss: 0.2065 - accuracy: 0.5894 - val_loss: 0.1972 - val_accuracy: 0.6189\n",
      "Epoch 2/10\n",
      "5625/5625 [==============================] - 229s 41ms/step - loss: 0.1834 - accuracy: 0.6522 - val_loss: 0.1875 - val_accuracy: 0.6534\n",
      "Epoch 3/10\n",
      "5625/5625 [==============================] - 226s 40ms/step - loss: 0.1739 - accuracy: 0.6751 - val_loss: 0.1814 - val_accuracy: 0.6565\n",
      "Epoch 4/10\n",
      "5625/5625 [==============================] - 221s 39ms/step - loss: 0.1672 - accuracy: 0.6897 - val_loss: 0.1799 - val_accuracy: 0.6647\n",
      "Epoch 5/10\n",
      "5625/5625 [==============================] - 220s 39ms/step - loss: 0.1631 - accuracy: 0.6998 - val_loss: 0.1924 - val_accuracy: 0.6413\n",
      "Epoch 6/10\n",
      "5625/5625 [==============================] - 220s 39ms/step - loss: 0.1600 - accuracy: 0.7061 - val_loss: 0.1761 - val_accuracy: 0.6756\n",
      "Epoch 7/10\n",
      "5625/5625 [==============================] - 220s 39ms/step - loss: 0.1574 - accuracy: 0.7104 - val_loss: 0.1787 - val_accuracy: 0.6681\n",
      "Epoch 8/10\n",
      "5625/5625 [==============================] - 227s 40ms/step - loss: 0.1544 - accuracy: 0.7173 - val_loss: 0.1767 - val_accuracy: 0.6708\n",
      "Epoch 9/10\n",
      "5625/5625 [==============================] - 290s 52ms/step - loss: 0.1514 - accuracy: 0.7235 - val_loss: 0.1850 - val_accuracy: 0.6586\n",
      "Epoch 10/10\n",
      "5625/5625 [==============================] - 263s 47ms/step - loss: 0.1495 - accuracy: 0.7267 - val_loss: 0.1806 - val_accuracy: 0.6436\n"
     ]
    }
   ],
   "source": [
    "early_stop = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n",
    "\n",
    "# fit the model\n",
    "history = model\n",
    "@jit\n",
    "def modelRNN():\n",
    "    history.fit(x=X_train,\n",
    "         y=Y_train,\n",
    "         epochs=10,\n",
    "         validation_data=(X_test, Y_test), verbose=1,\n",
    "         callbacks=[early_stop]\n",
    "         )\n",
    "modelRNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/63 [====>.........................] - ETA: 1:26"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/63 [=======>......................] - ETA: 1:19"
     ]
    }
   ],
   "source": [
    "preds = trained_model.predict([val_inp,val_mask],batch_size=32)\n",
    "pred_labels = preds.argmax(axis=1)\n",
    "print('Classification Report')\n",
    "print(classification_report(val_label,pred_labels))\n",
    "\n",
    "print('Training and saving built model.....')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.90      0.90      0.90       393\n",
      "           2       0.92      0.96      0.94       420\n",
      "           3       0.95      0.94      0.95       400\n",
      "           4       0.89      0.88      0.89       427\n",
      "           5       0.97      0.96      0.96       406\n",
      "           6       0.96      0.97      0.96       378\n",
      "           7       0.88      0.86      0.87       390\n",
      "           8       0.95      0.91      0.93       388\n",
      "           9       0.93      0.95      0.94       407\n",
      "          10       0.93      0.95      0.94       391\n",
      "\n",
      "    accuracy                           0.93      4000\n",
      "   macro avg       0.93      0.93      0.93      4000\n",
      "weighted avg       0.93      0.93      0.93      4000\n",
      "\n",
      "Training and saving built model.....\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, ..., 1, 1, 1],\n",
       "       [1, 1, 1, ..., 1, 1, 1],\n",
       "       [1, 1, 1, ..., 1, 1, 1],\n",
       "       ...,\n",
       "       [1, 1, 1, ..., 1, 1, 1],\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 1, 1, 1]])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_mask"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
