{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "# Copyright 2018 The HuggingFace Inc. team.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\"\"\"Tokenization classes for DistilBERT.\"\"\"\n",
    "\n",
    "import collections\n",
    "import os\n",
    "import unicodedata\n",
    "from typing import List, Optional, Tuple\n",
    "\n",
    "from ...tokenization_utils import PreTrainedTokenizer, _is_control, _is_punctuation, _is_whitespace\n",
    "from ...utils import logging\n",
    "\n",
    "\n",
    "logger = logging.get_logger(__name__)\n",
    "\n",
    "VOCAB_FILES_NAMES = {\"vocab_file\": \"vocab.txt\"}\n",
    "\n",
    "PRETRAINED_VOCAB_FILES_MAP = {\n",
    "    \"vocab_file\": {\n",
    "        \"distilbert-base-uncased\": \"https://huggingface.co/distilbert-base-uncased/resolve/main/vocab.txt\",\n",
    "        \"distilbert-base-uncased-distilled-squad\": (\n",
    "            \"https://huggingface.co/distilbert-base-uncased-distilled-squad/resolve/main/vocab.txt\"\n",
    "        ),\n",
    "        \"distilbert-base-cased\": \"https://huggingface.co/distilbert-base-cased/resolve/main/vocab.txt\",\n",
    "        \"distilbert-base-cased-distilled-squad\": (\n",
    "            \"https://huggingface.co/distilbert-base-cased-distilled-squad/resolve/main/vocab.txt\"\n",
    "        ),\n",
    "        \"distilbert-base-german-cased\": \"https://huggingface.co/distilbert-base-german-cased/resolve/main/vocab.txt\",\n",
    "        \"distilbert-base-multilingual-cased\": (\n",
    "            \"https://huggingface.co/distilbert-base-multilingual-cased/resolve/main/vocab.txt\"\n",
    "        ),\n",
    "    }\n",
    "}\n",
    "\n",
    "PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES = {\n",
    "    \"distilbert-base-uncased\": 512,\n",
    "    \"distilbert-base-uncased-distilled-squad\": 512,\n",
    "    \"distilbert-base-cased\": 512,\n",
    "    \"distilbert-base-cased-distilled-squad\": 512,\n",
    "    \"distilbert-base-german-cased\": 512,\n",
    "    \"distilbert-base-multilingual-cased\": 512,\n",
    "}\n",
    "\n",
    "\n",
    "PRETRAINED_INIT_CONFIGURATION = {\n",
    "    \"distilbert-base-uncased\": {\"do_lower_case\": True},\n",
    "    \"distilbert-base-uncased-distilled-squad\": {\"do_lower_case\": True},\n",
    "    \"distilbert-base-cased\": {\"do_lower_case\": False},\n",
    "    \"distilbert-base-cased-distilled-squad\": {\"do_lower_case\": False},\n",
    "    \"distilbert-base-german-cased\": {\"do_lower_case\": False},\n",
    "    \"distilbert-base-multilingual-cased\": {\"do_lower_case\": False},\n",
    "}\n",
    "\n",
    "\n",
    "# Copied from transformers.models.bert.tokenization_bert.load_vocab\n",
    "def load_vocab(vocab_file):\n",
    "    \"\"\"Loads a vocabulary file into a dictionary.\"\"\"\n",
    "    vocab = collections.OrderedDict()\n",
    "    with open(vocab_file, \"r\", encoding=\"utf-8\") as reader:\n",
    "        tokens = reader.readlines()\n",
    "    for index, token in enumerate(tokens):\n",
    "        token = token.rstrip(\"\\n\")\n",
    "        vocab[token] = index\n",
    "    return vocab\n",
    "\n",
    "\n",
    "# Copied from transformers.models.bert.tokenization_bert.whitespace_tokenize\n",
    "def whitespace_tokenize(text):\n",
    "    \"\"\"Runs basic whitespace cleaning and splitting on a piece of text.\"\"\"\n",
    "    text = text.strip()\n",
    "    if not text:\n",
    "        return []\n",
    "    tokens = text.split()\n",
    "    return tokens\n",
    "\n",
    "\n",
    "class DistilBertTokenizer(PreTrainedTokenizer):\n",
    "    r\"\"\"\n",
    "    Construct a DistilBERT tokenizer. Based on WordPiece.\n",
    "\n",
    "    This tokenizer inherits from [`PreTrainedTokenizer`] which contains most of the main methods. Users should refer to\n",
    "    this superclass for more information regarding those methods.\n",
    "\n",
    "    Args:\n",
    "        vocab_file (`str`):\n",
    "            File containing the vocabulary.\n",
    "        do_lower_case (`bool`, *optional*, defaults to `True`):\n",
    "            Whether or not to lowercase the input when tokenizing.\n",
    "        do_basic_tokenize (`bool`, *optional*, defaults to `True`):\n",
    "            Whether or not to do basic tokenization before WordPiece.\n",
    "        never_split (`Iterable`, *optional*):\n",
    "            Collection of tokens which will never be split during tokenization. Only has an effect when\n",
    "            `do_basic_tokenize=True`\n",
    "        unk_token (`str`, *optional*, defaults to `\"[UNK]\"`):\n",
    "            The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this\n",
    "            token instead.\n",
    "        sep_token (`str`, *optional*, defaults to `\"[SEP]\"`):\n",
    "            The separator token, which is used when building a sequence from multiple sequences, e.g. two sequences for\n",
    "            sequence classification or for a text and a question for question answering. It is also used as the last\n",
    "            token of a sequence built with special tokens.\n",
    "        pad_token (`str`, *optional*, defaults to `\"[PAD]\"`):\n",
    "            The token used for padding, for example when batching sequences of different lengths.\n",
    "        cls_token (`str`, *optional*, defaults to `\"[CLS]\"`):\n",
    "            The classifier token which is used when doing sequence classification (classification of the whole sequence\n",
    "            instead of per-token classification). It is the first token of the sequence when built with special tokens.\n",
    "        mask_token (`str`, *optional*, defaults to `\"[MASK]\"`):\n",
    "            The token used for masking values. This is the token used when training this model with masked language\n",
    "            modeling. This is the token which the model will try to predict.\n",
    "        tokenize_chinese_chars (`bool`, *optional*, defaults to `True`):\n",
    "            Whether or not to tokenize Chinese characters.\n",
    "\n",
    "            This should likely be deactivated for Japanese (see this\n",
    "            [issue](https://github.com/huggingface/transformers/issues/328)).\n",
    "        strip_accents (`bool`, *optional*):\n",
    "            Whether or not to strip all accents. If this option is not specified, then it will be determined by the\n",
    "            value for `lowercase` (as in the original BERT).\n",
    "    \"\"\"\n",
    "\n",
    "    vocab_files_names = VOCAB_FILES_NAMES\n",
    "    pretrained_vocab_files_map = PRETRAINED_VOCAB_FILES_MAP\n",
    "    pretrained_init_configuration = PRETRAINED_INIT_CONFIGURATION\n",
    "    max_model_input_sizes = PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES\n",
    "    model_input_names = [\"input_ids\", \"attention_mask\"]\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_file,\n",
    "        do_lower_case=True,\n",
    "        do_basic_tokenize=True,\n",
    "        never_split=None,\n",
    "        unk_token=\"[UNK]\",\n",
    "        sep_token=\"[SEP]\",\n",
    "        pad_token=\"[PAD]\",\n",
    "        cls_token=\"[CLS]\",\n",
    "        mask_token=\"[MASK]\",\n",
    "        tokenize_chinese_chars=True,\n",
    "        strip_accents=None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            do_lower_case=do_lower_case,\n",
    "            do_basic_tokenize=do_basic_tokenize,\n",
    "            never_split=never_split,\n",
    "            unk_token=unk_token,\n",
    "            sep_token=sep_token,\n",
    "            pad_token=pad_token,\n",
    "            cls_token=cls_token,\n",
    "            mask_token=mask_token,\n",
    "            tokenize_chinese_chars=tokenize_chinese_chars,\n",
    "            strip_accents=strip_accents,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        if not os.path.isfile(vocab_file):\n",
    "            raise ValueError(\n",
    "                f\"Can't find a vocabulary file at path '{vocab_file}'. To load the vocabulary from a Google pretrained\"\n",
    "                \" model use `tokenizer = DistilBertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)`\"\n",
    "            )\n",
    "        self.vocab = load_vocab(vocab_file)\n",
    "        self.ids_to_tokens = collections.OrderedDict([(ids, tok) for tok, ids in self.vocab.items()])\n",
    "        self.do_basic_tokenize = do_basic_tokenize\n",
    "        if do_basic_tokenize:\n",
    "            self.basic_tokenizer = BasicTokenizer(\n",
    "                do_lower_case=do_lower_case,\n",
    "                never_split=never_split,\n",
    "                tokenize_chinese_chars=tokenize_chinese_chars,\n",
    "                strip_accents=strip_accents,\n",
    "            )\n",
    "        self.wordpiece_tokenizer = WordpieceTokenizer(vocab=self.vocab, unk_token=self.unk_token)\n",
    "\n",
    "    @property\n",
    "    # Copied from transformers.models.bert.tokenization_bert.BertTokenizer.do_lower_case\n",
    "    def do_lower_case(self):\n",
    "        return self.basic_tokenizer.do_lower_case\n",
    "\n",
    "    @property\n",
    "    # Copied from transformers.models.bert.tokenization_bert.BertTokenizer.vocab_size\n",
    "    def vocab_size(self):\n",
    "        return len(self.vocab)\n",
    "\n",
    "    # Copied from transformers.models.bert.tokenization_bert.BertTokenizer.get_vocab\n",
    "    def get_vocab(self):\n",
    "        return dict(self.vocab, **self.added_tokens_encoder)\n",
    "\n",
    "    # Copied from transformers.models.bert.tokenization_bert.BertTokenizer._tokenize\n",
    "    def _tokenize(self, text):\n",
    "        split_tokens = []\n",
    "        if self.do_basic_tokenize:\n",
    "            for token in self.basic_tokenizer.tokenize(text, never_split=self.all_special_tokens):\n",
    "                # If the token is part of the never_split set\n",
    "                if token in self.basic_tokenizer.never_split:\n",
    "                    split_tokens.append(token)\n",
    "                else:\n",
    "                    split_tokens += self.wordpiece_tokenizer.tokenize(token)\n",
    "        else:\n",
    "            split_tokens = self.wordpiece_tokenizer.tokenize(text)\n",
    "        return split_tokens\n",
    "\n",
    "    # Copied from transformers.models.bert.tokenization_bert.BertTokenizer._convert_token_to_id\n",
    "    def _convert_token_to_id(self, token):\n",
    "        \"\"\"Converts a token (str) in an id using the vocab.\"\"\"\n",
    "        return self.vocab.get(token, self.vocab.get(self.unk_token))\n",
    "\n",
    "    # Copied from transformers.models.bert.tokenization_bert.BertTokenizer._convert_id_to_token\n",
    "    def _convert_id_to_token(self, index):\n",
    "        \"\"\"Converts an index (integer) in a token (str) using the vocab.\"\"\"\n",
    "        return self.ids_to_tokens.get(index, self.unk_token)\n",
    "\n",
    "    # Copied from transformers.models.bert.tokenization_bert.BertTokenizer.convert_tokens_to_string\n",
    "    def convert_tokens_to_string(self, tokens):\n",
    "        \"\"\"Converts a sequence of tokens (string) in a single string.\"\"\"\n",
    "        out_string = \" \".join(tokens).replace(\" ##\", \"\").strip()\n",
    "        return out_string\n",
    "\n",
    "    # Copied from transformers.models.bert.tokenization_bert.BertTokenizer.build_inputs_with_special_tokens\n",
    "    def build_inputs_with_special_tokens(\n",
    "        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n",
    "    ) -> List[int]:\n",
    "        \"\"\"\n",
    "        Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and\n",
    "        adding special tokens. A BERT sequence has the following format:\n",
    "\n",
    "        - single sequence: `[CLS] X [SEP]`\n",
    "        - pair of sequences: `[CLS] A [SEP] B [SEP]`\n",
    "\n",
    "        Args:\n",
    "            token_ids_0 (`List[int]`):\n",
    "                List of IDs to which the special tokens will be added.\n",
    "            token_ids_1 (`List[int]`, *optional*):\n",
    "                Optional second list of IDs for sequence pairs.\n",
    "\n",
    "        Returns:\n",
    "            `List[int]`: List of [input IDs](../glossary#input-ids) with the appropriate special tokens.\n",
    "        \"\"\"\n",
    "        if token_ids_1 is None:\n",
    "            return [self.cls_token_id] + token_ids_0 + [self.sep_token_id]\n",
    "        cls = [self.cls_token_id]\n",
    "        sep = [self.sep_token_id]\n",
    "        return cls + token_ids_0 + sep + token_ids_1 + sep\n",
    "\n",
    "    # Copied from transformers.models.bert.tokenization_bert.BertTokenizer.get_special_tokens_mask\n",
    "    def get_special_tokens_mask(\n",
    "        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None, already_has_special_tokens: bool = False\n",
    "    ) -> List[int]:\n",
    "        \"\"\"\n",
    "        Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding\n",
    "        special tokens using the tokenizer `prepare_for_model` method.\n",
    "\n",
    "        Args:\n",
    "            token_ids_0 (`List[int]`):\n",
    "                List of IDs.\n",
    "            token_ids_1 (`List[int]`, *optional*):\n",
    "                Optional second list of IDs for sequence pairs.\n",
    "            already_has_special_tokens (`bool`, *optional*, defaults to `False`):\n",
    "                Whether or not the token list is already formatted with special tokens for the model.\n",
    "\n",
    "        Returns:\n",
    "            `List[int]`: A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.\n",
    "        \"\"\"\n",
    "\n",
    "        if already_has_special_tokens:\n",
    "            return super().get_special_tokens_mask(\n",
    "                token_ids_0=token_ids_0, token_ids_1=token_ids_1, already_has_special_tokens=True\n",
    "            )\n",
    "\n",
    "        if token_ids_1 is not None:\n",
    "            return [1] + ([0] * len(token_ids_0)) + [1] + ([0] * len(token_ids_1)) + [1]\n",
    "        return [1] + ([0] * len(token_ids_0)) + [1]\n",
    "\n",
    "    # Copied from transformers.models.bert.tokenization_bert.BertTokenizer.create_token_type_ids_from_sequences\n",
    "    def create_token_type_ids_from_sequences(\n",
    "        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n",
    "    ) -> List[int]:\n",
    "        \"\"\"\n",
    "        Create a mask from the two sequences passed to be used in a sequence-pair classification task. A BERT sequence\n",
    "        pair mask has the following format:\n",
    "\n",
    "        ```\n",
    "        0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1\n",
    "        | first sequence    | second sequence |\n",
    "        ```\n",
    "\n",
    "        If `token_ids_1` is `None`, this method only returns the first portion of the mask (0s).\n",
    "\n",
    "        Args:\n",
    "            token_ids_0 (`List[int]`):\n",
    "                List of IDs.\n",
    "            token_ids_1 (`List[int]`, *optional*):\n",
    "                Optional second list of IDs for sequence pairs.\n",
    "\n",
    "        Returns:\n",
    "            `List[int]`: List of [token type IDs](../glossary#token-type-ids) according to the given sequence(s).\n",
    "        \"\"\"\n",
    "        sep = [self.sep_token_id]\n",
    "        cls = [self.cls_token_id]\n",
    "        if token_ids_1 is None:\n",
    "            return len(cls + token_ids_0 + sep) * [0]\n",
    "        return len(cls + token_ids_0 + sep) * [0] + len(token_ids_1 + sep) * [1]\n",
    "\n",
    "    # Copied from transformers.models.bert.tokenization_bert.BertTokenizer.save_vocabulary\n",
    "    def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> Tuple[str]:\n",
    "        index = 0\n",
    "        if os.path.isdir(save_directory):\n",
    "            vocab_file = os.path.join(\n",
    "                save_directory, (filename_prefix + \"-\" if filename_prefix else \"\") + VOCAB_FILES_NAMES[\"vocab_file\"]\n",
    "            )\n",
    "        else:\n",
    "            vocab_file = (filename_prefix + \"-\" if filename_prefix else \"\") + save_directory\n",
    "        with open(vocab_file, \"w\", encoding=\"utf-8\") as writer:\n",
    "            for token, token_index in sorted(self.vocab.items(), key=lambda kv: kv[1]):\n",
    "                if index != token_index:\n",
    "                    logger.warning(\n",
    "                        f\"Saving vocabulary to {vocab_file}: vocabulary indices are not consecutive.\"\n",
    "                        \" Please check that the vocabulary is not corrupted!\"\n",
    "                    )\n",
    "                    index = token_index\n",
    "                writer.write(token + \"\\n\")\n",
    "                index += 1\n",
    "        return (vocab_file,)\n",
    "\n",
    "\n",
    "# Copied from transformers.models.bert.tokenization_bert.BasicTokenizer\n",
    "class BasicTokenizer(object):\n",
    "    \"\"\"\n",
    "    Constructs a BasicTokenizer that will run basic tokenization (punctuation splitting, lower casing, etc.).\n",
    "\n",
    "    Args:\n",
    "        do_lower_case (`bool`, *optional*, defaults to `True`):\n",
    "            Whether or not to lowercase the input when tokenizing.\n",
    "        never_split (`Iterable`, *optional*):\n",
    "            Collection of tokens which will never be split during tokenization. Only has an effect when\n",
    "            `do_basic_tokenize=True`\n",
    "        tokenize_chinese_chars (`bool`, *optional*, defaults to `True`):\n",
    "            Whether or not to tokenize Chinese characters.\n",
    "\n",
    "            This should likely be deactivated for Japanese (see this\n",
    "            [issue](https://github.com/huggingface/transformers/issues/328)).\n",
    "        strip_accents (`bool`, *optional*):\n",
    "            Whether or not to strip all accents. If this option is not specified, then it will be determined by the\n",
    "            value for `lowercase` (as in the original BERT).\n",
    "        do_split_on_punc (`bool`, *optional*, defaults to `True`):\n",
    "            In some instances we want to skip the basic punctuation splitting so that later tokenization can capture\n",
    "            the full context of the words, such as contractions.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        do_lower_case=True,\n",
    "        never_split=None,\n",
    "        tokenize_chinese_chars=True,\n",
    "        strip_accents=None,\n",
    "        do_split_on_punc=True,\n",
    "    ):\n",
    "        if never_split is None:\n",
    "            never_split = []\n",
    "        self.do_lower_case = do_lower_case\n",
    "        self.never_split = set(never_split)\n",
    "        self.tokenize_chinese_chars = tokenize_chinese_chars\n",
    "        self.strip_accents = strip_accents\n",
    "        self.do_split_on_punc = do_split_on_punc\n",
    "\n",
    "    def tokenize(self, text, never_split=None):\n",
    "        \"\"\"\n",
    "        Basic Tokenization of a piece of text. For sub-word tokenization, see WordPieceTokenizer.\n",
    "\n",
    "        Args:\n",
    "            never_split (`List[str]`, *optional*)\n",
    "                Kept for backward compatibility purposes. Now implemented directly at the base class level (see\n",
    "                [`PreTrainedTokenizer.tokenize`]) List of token not to split.\n",
    "        \"\"\"\n",
    "        # union() returns a new set by concatenating the two sets.\n",
    "        never_split = self.never_split.union(set(never_split)) if never_split else self.never_split\n",
    "        text = self._clean_text(text)\n",
    "\n",
    "        # This was added on November 1st, 2018 for the multilingual and Chinese\n",
    "        # models. This is also applied to the English models now, but it doesn't\n",
    "        # matter since the English models were not trained on any Chinese data\n",
    "        # and generally don't have any Chinese data in them (there are Chinese\n",
    "        # characters in the vocabulary because Wikipedia does have some Chinese\n",
    "        # words in the English Wikipedia.).\n",
    "        if self.tokenize_chinese_chars:\n",
    "            text = self._tokenize_chinese_chars(text)\n",
    "        # prevents treating the same character with different unicode codepoints as different characters\n",
    "        unicode_normalized_text = unicodedata.normalize(\"NFC\", text)\n",
    "        orig_tokens = whitespace_tokenize(unicode_normalized_text)\n",
    "        split_tokens = []\n",
    "        for token in orig_tokens:\n",
    "            if token not in never_split:\n",
    "                if self.do_lower_case:\n",
    "                    token = token.lower()\n",
    "                    if self.strip_accents is not False:\n",
    "                        token = self._run_strip_accents(token)\n",
    "                elif self.strip_accents:\n",
    "                    token = self._run_strip_accents(token)\n",
    "            split_tokens.extend(self._run_split_on_punc(token, never_split))\n",
    "\n",
    "        output_tokens = whitespace_tokenize(\" \".join(split_tokens))\n",
    "        return output_tokens\n",
    "\n",
    "    def _run_strip_accents(self, text):\n",
    "        \"\"\"Strips accents from a piece of text.\"\"\"\n",
    "        text = unicodedata.normalize(\"NFD\", text)\n",
    "        output = []\n",
    "        for char in text:\n",
    "            cat = unicodedata.category(char)\n",
    "            if cat == \"Mn\":\n",
    "                continue\n",
    "            output.append(char)\n",
    "        return \"\".join(output)\n",
    "\n",
    "    def _run_split_on_punc(self, text, never_split=None):\n",
    "        \"\"\"Splits punctuation on a piece of text.\"\"\"\n",
    "        if not self.do_split_on_punc or (never_split is not None and text in never_split):\n",
    "            return [text]\n",
    "        chars = list(text)\n",
    "        i = 0\n",
    "        start_new_word = True\n",
    "        output = []\n",
    "        while i < len(chars):\n",
    "            char = chars[i]\n",
    "            if _is_punctuation(char):\n",
    "                output.append([char])\n",
    "                start_new_word = True\n",
    "            else:\n",
    "                if start_new_word:\n",
    "                    output.append([])\n",
    "                start_new_word = False\n",
    "                output[-1].append(char)\n",
    "            i += 1\n",
    "\n",
    "        return [\"\".join(x) for x in output]\n",
    "\n",
    "    def _tokenize_chinese_chars(self, text):\n",
    "        \"\"\"Adds whitespace around any CJK character.\"\"\"\n",
    "        output = []\n",
    "        for char in text:\n",
    "            cp = ord(char)\n",
    "            if self._is_chinese_char(cp):\n",
    "                output.append(\" \")\n",
    "                output.append(char)\n",
    "                output.append(\" \")\n",
    "            else:\n",
    "                output.append(char)\n",
    "        return \"\".join(output)\n",
    "\n",
    "    def _is_chinese_char(self, cp):\n",
    "        \"\"\"Checks whether CP is the codepoint of a CJK character.\"\"\"\n",
    "        # This defines a \"chinese character\" as anything in the CJK Unicode block:\n",
    "        #   https://en.wikipedia.org/wiki/CJK_Unified_Ideographs_(Unicode_block)\n",
    "        #\n",
    "        # Note that the CJK Unicode block is NOT all Japanese and Korean characters,\n",
    "        # despite its name. The modern Korean Hangul alphabet is a different block,\n",
    "        # as is Japanese Hiragana and Katakana. Those alphabets are used to write\n",
    "        # space-separated words, so they are not treated specially and handled\n",
    "        # like the all of the other languages.\n",
    "        if (\n",
    "            (cp >= 0x4E00 and cp <= 0x9FFF)\n",
    "            or (cp >= 0x3400 and cp <= 0x4DBF)  #\n",
    "            or (cp >= 0x20000 and cp <= 0x2A6DF)  #\n",
    "            or (cp >= 0x2A700 and cp <= 0x2B73F)  #\n",
    "            or (cp >= 0x2B740 and cp <= 0x2B81F)  #\n",
    "            or (cp >= 0x2B820 and cp <= 0x2CEAF)  #\n",
    "            or (cp >= 0xF900 and cp <= 0xFAFF)\n",
    "            or (cp >= 0x2F800 and cp <= 0x2FA1F)  #\n",
    "        ):  #\n",
    "            return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    def _clean_text(self, text):\n",
    "        \"\"\"Performs invalid character removal and whitespace cleanup on text.\"\"\"\n",
    "        output = []\n",
    "        for char in text:\n",
    "            cp = ord(char)\n",
    "            if cp == 0 or cp == 0xFFFD or _is_control(char):\n",
    "                continue\n",
    "            if _is_whitespace(char):\n",
    "                output.append(\" \")\n",
    "            else:\n",
    "                output.append(char)\n",
    "        return \"\".join(output)\n",
    "\n",
    "\n",
    "# Copied from transformers.models.bert.tokenization_bert.WordpieceTokenizer\n",
    "class WordpieceTokenizer(object):\n",
    "    \"\"\"Runs WordPiece tokenization.\"\"\"\n",
    "\n",
    "    def __init__(self, vocab, unk_token, max_input_chars_per_word=100):\n",
    "        self.vocab = vocab\n",
    "        self.unk_token = unk_token\n",
    "        self.max_input_chars_per_word = max_input_chars_per_word\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        \"\"\"\n",
    "        Tokenizes a piece of text into its word pieces. This uses a greedy longest-match-first algorithm to perform\n",
    "        tokenization using the given vocabulary.\n",
    "\n",
    "        For example, `input = \"unaffable\"` wil return as output `[\"un\", \"##aff\", \"##able\"]`.\n",
    "\n",
    "        Args:\n",
    "            text: A single token or whitespace separated tokens. This should have\n",
    "                already been passed through *BasicTokenizer*.\n",
    "\n",
    "        Returns:\n",
    "            A list of wordpiece tokens.\n",
    "        \"\"\"\n",
    "\n",
    "        output_tokens = []\n",
    "        for token in whitespace_tokenize(text):\n",
    "            chars = list(token)\n",
    "            if len(chars) > self.max_input_chars_per_word:\n",
    "                output_tokens.append(self.unk_token)\n",
    "                continue\n",
    "\n",
    "            is_bad = False\n",
    "            start = 0\n",
    "            sub_tokens = []\n",
    "            while start < len(chars):\n",
    "                end = len(chars)\n",
    "                cur_substr = None\n",
    "                while start < end:\n",
    "                    substr = \"\".join(chars[start:end])\n",
    "                    if start > 0:\n",
    "                        substr = \"##\" + substr\n",
    "                    if substr in self.vocab:\n",
    "                        cur_substr = substr\n",
    "                        break\n",
    "                    end -= 1\n",
    "                if cur_substr is None:\n",
    "                    is_bad = True\n",
    "                    break\n",
    "                sub_tokens.append(cur_substr)\n",
    "                start = end\n",
    "\n",
    "            if is_bad:\n",
    "                output_tokens.append(self.unk_token)\n",
    "            else:\n",
    "                output_tokens.extend(sub_tokens)\n",
    "        return output_tokens"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
